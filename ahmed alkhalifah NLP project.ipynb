{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "andA09XoSLF2",
    "outputId": "9db43d7c-c298-44d3-d451-2e29e1980593"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in /usr/local/lib/python3.6/dist-packages (0.0.25)\n",
      "Requirement already satisfied: textsearch in /usr/local/lib/python3.6/dist-packages (from contractions) (0.0.17)\n",
      "Requirement already satisfied: Unidecode in /usr/local/lib/python3.6/dist-packages (from textsearch->contractions) (1.1.1)\n",
      "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.6/dist-packages (from textsearch->contractions) (1.4.0)\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install contractions\n",
    "\n",
    "import re, string, unicodedata                          # Import Regex, string and unicodedata.\n",
    "import contractions                                     # Import contractions library.\n",
    "from bs4 import BeautifulSoup                           # Import BeautifulSoup.\n",
    "\n",
    "import numpy as np                                      # Import numpy.\n",
    "import pandas as pd                                     # Import pandas.\n",
    "import nltk                                             # Import Natural Language Tool-Kit.\n",
    "\n",
    "nltk.download('stopwords')                              # Download Stopwords.\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords                       # Import stopwords.\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize  # Import Tokenizer.\n",
    "from nltk.stem.wordnet import WordNetLemmatizer         # Import Lemmatizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "tEwb83IzbfK_",
    "outputId": "789bb92e-a2b7-42e7-f23c-d01120aedc66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "# drive.mount('/gdrive')\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ndzu6qmXc6Ln"
   },
   "outputs": [],
   "source": [
    "alldata = pd.read_csv('/content/drive/My Drive/Tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "WwRngq0dc6ku",
    "outputId": "7e1f04e7-9741-41f9-cbc8-df0a86b20de5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tweet_id', 'airline_sentiment', 'airline_sentiment_confidence',\n",
       "       'negativereason', 'negativereason_confidence', 'airline',\n",
       "       'airline_sentiment_gold', 'name', 'negativereason_gold',\n",
       "       'retweet_count', 'text', 'tweet_coord', 'tweet_created',\n",
       "       'tweet_location', 'user_timezone'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alldata.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dn2Nmy3Hc6Xy"
   },
   "outputs": [],
   "source": [
    "data = alldata[['airline_sentiment','text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "C256UtPAc6eF",
    "outputId": "5f752080-3e8e-45b4-8643-9ccfd83d0e94"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14640, 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "7ATY6PCpc6Gv",
    "outputId": "dd1ff1d7-e3b5-4001-fe9e-1143a6376663"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment                                               text\n",
       "0           neutral                @VirginAmerica What @dhepburn said.\n",
       "1          positive  @VirginAmerica plus you've added commercials t...\n",
       "2           neutral  @VirginAmerica I didn't today... Must mean I n...\n",
       "3          negative  @VirginAmerica it's really aggressive to blast...\n",
       "4          negative  @VirginAmerica and it's a really big bad thing..."
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "SMYKtvqcgFbI",
    "outputId": "d4f8a9cd-082f-47a0-95c9-c0edf1536624"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "airline_sentiment    0\n",
       "text                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aVkM62r0hyPv"
   },
   "outputs": [],
   "source": [
    "# Making a list of stop words\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "customlist = ['not', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn',\n",
    "        \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\n",
    "        \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn',\n",
    "        \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "\n",
    "stopwords = list(set(stopwords) - set(customlist))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "IX8sb3OdrFWi",
    "outputId": "06db1f14-1da1-424b-e7c3-3035bf9c0bfb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>virginamerica dhepburn say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>virginamerica plus add commercials experience ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>virginamerica not today must mean need take an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>virginamerica really aggressive blast obnoxiou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>virginamerica really big bad thing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment                                               text\n",
       "0           neutral                         virginamerica dhepburn say\n",
       "1          positive  virginamerica plus add commercials experience ...\n",
       "2           neutral  virginamerica not today must mean need take an...\n",
       "3          negative  virginamerica really aggressive blast obnoxiou...\n",
       "4          negative                 virginamerica really big bad thing"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data preprocessing\n",
    "# -----------------------------------\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def tweetes_preprocessing(words):\n",
    "\n",
    "# HTML Tag removal\n",
    "  soup = BeautifulSoup(words, \"html.parser\")\n",
    "  no_html = soup.get_text()\n",
    "\n",
    "# Remove contractions\n",
    "  fixed_contractions = contractions.fix(no_html)\n",
    "\n",
    "# Remove numbers and keep only text\n",
    "  letters_only = re.sub(\"[^a-zA-Z]\", \" \", fixed_contractions)\n",
    "\n",
    "# Convert to small letters\n",
    "  words = letters_only.lower()\n",
    "\n",
    "# Tokenize words \n",
    "  words = nltk.word_tokenize(words)\n",
    "\n",
    "#  Remove non-ASCII characters from list of tokenized words\n",
    "  non_ASCII = []\n",
    "  for word in words:\n",
    "    new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    non_ASCII.append(new_word)\n",
    "\n",
    "# Remove punctuation from list of tokenized words\n",
    "  no_punctuation = []\n",
    "  for word in non_ASCII:\n",
    "    new_word = re.sub(r'[.|,|)|(|\\|/]',r' ', word)\n",
    "    if new_word != '':\n",
    "        no_punctuation.append(new_word)\n",
    "\n",
    "# Remove stop words from list of tokenized words\n",
    "  no_stopwords = []\n",
    "  for word in no_punctuation:\n",
    "    if word not in stopwords:\n",
    "        no_stopwords.append(word)\n",
    "\n",
    "# Lemmetize words\n",
    "  lemmatized_words = []\n",
    "  for word in no_stopwords:\n",
    "    lemmatized_words.append(lemmatizer.lemmatize(word, pos='v'))\n",
    "  \n",
    "  return  (\" \".join(lemmatized_words))\n",
    "\n",
    "# -----------------------------------\n",
    "data['text'] = data['text'].apply(lambda x: tweetes_preprocessing(x))\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "5Ak_EzyWzMqW",
    "outputId": "86dcad24-6753-407a-e9d2-769acdd73256"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neutral' 'positive' 'negative']\n",
      "[1 2 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "print (data['airline_sentiment'].unique())\n",
    "data['airline_sentiment']= data['airline_sentiment'].replace({'neutral':1, 'positive':2, 'negative':0})\n",
    "print (data['airline_sentiment'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ToE_7vfFN2ao"
   },
   "outputs": [],
   "source": [
    "labels = data['airline_sentiment']\n",
    "labels = labels.astype('int')\n",
    "data_features = data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xx_ZDU470yFY"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "v = CountVectorizer(analyzer = \"word\", max_df=1.0, min_df=1, max_features=None)\n",
    "data_features= v.fit_transform(data_features)\n",
    "data_features=data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A86mBvFtjDE9"
   },
   "outputs": [],
   "source": [
    "# Split data into training and testing set.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_features, labels, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "7QOTv0uj04vP",
    "outputId": "0738a811-fa5f-44ba-df62-c04e6c3cca5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=4,\n",
      "                       oob_score=False, random_state=None, verbose=0,\n",
      "                       warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=200, n_jobs=4)\n",
    "\n",
    "forest = forest.fit(X_train, y_train)\n",
    "\n",
    "print(forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "NEvv7dMfDNqx",
    "outputId": "e47a32db-5797-4fc0-e238-38b15de3f917"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7430327868852459\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "print(np.mean(cross_val_score(forest, data_features, labels, cv=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "id": "UzJ6W1Y-04jR",
    "outputId": "f5ad82a5-f25b-4dda-c5c4-b43ec981d5b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data score: 0.9961943793911007\n",
      "Test score: 0.7830145719489982\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2594</td>\n",
       "      <td>160</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>377</td>\n",
       "      <td>426</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>179</td>\n",
       "      <td>96</td>\n",
       "      <td>419</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted     0    1    2\n",
       "Actual                   \n",
       "0          2594  160   60\n",
       "1           377  426   81\n",
       "2           179   96  419"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "y_predict = forest.predict(X_test)\n",
    "train= forest.score(X_train, y_train)\n",
    "test= forest.score(X_test, y_test)\n",
    "modelaccuracy=accuracy_score(y_test, y_predict)\n",
    "print ('Train data score:',train)\n",
    "print ('Test score:',test)\n",
    "pd.crosstab(y_test, y_predict, rownames=['Actual'], colnames=['Predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UZ-MmdBTAyeY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z1A926jOAyva"
   },
   "outputs": [],
   "source": [
    "# Using TfidfVectorizer to convert text data to numbers.\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "data_features = data['text']\n",
    "vectorizer = TfidfVectorizer(analyzer = \"word\", max_df=1.0, min_df=1, max_features=None)\n",
    "data_features= vectorizer.fit_transform(data_features)\n",
    "data_features=data_features.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YESx0iEXQFwc"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data_features, labels, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "RH7U_OViAyb0",
    "outputId": "7f3d3c56-72b8-421d-cd6b-10bf90e0984b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=4,\n",
      "                       oob_score=False, random_state=None, verbose=0,\n",
      "                       warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=200, n_jobs=4)\n",
    "\n",
    "forest = forest.fit(X_train, y_train)\n",
    "\n",
    "print(forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "ZAXJQ66gQJWz",
    "outputId": "a0862292-6979-42e9-909f-1263625dc008"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7335382513661203\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(cross_val_score(forest, data_features, labels, cv=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "id": "aen_tUizDiHh",
    "outputId": "315c8d12-d00a-4282-d532-4744d2fb2631"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data score: 0.9961943793911007\n",
      "Test score: 0.7750455373406193\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2692</td>\n",
       "      <td>94</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>489</td>\n",
       "      <td>349</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>242</td>\n",
       "      <td>89</td>\n",
       "      <td>363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted     0    1    2\n",
       "Actual                   \n",
       "0          2692   94   28\n",
       "1           489  349   46\n",
       "2           242   89  363"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict = forest.predict(X_test)\n",
    "train= forest.score(X_train, y_train)\n",
    "test= forest.score(X_test, y_test)\n",
    "modelaccuracy=accuracy_score(y_test, y_predict)\n",
    "print ('Train data score:',train)\n",
    "print ('Test score:',test)\n",
    "pd.crosstab(y_test, y_predict, rownames=['Actual'], colnames=['Predicted'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R5e2Y3MaymWN"
   },
   "source": [
    "\n",
    "\n",
    "*   Initial data was raw data from twitter feeds and required multiple preprocessing techniques inculding:\n",
    "    *   HTML Tag removal: which removes the non meaningfull HTML programming codes.\n",
    "    *   Contraction removal: which changes appreviated word to its original form\n",
    "    *   Removing numbers: as they are not needed for this NLP model\n",
    "    *   Tokenize words: seperates words for inividual preprocessing\n",
    "    *   Changing all letters to small letters\n",
    "    *   Remove non-ASCII characters: to convert words with non understandable letters to basic english letters\n",
    "    *   Removing punctioations: removes dots and commas as not needed here\n",
    "    *   Removing stop words: not needed words for this NLP model\n",
    "    *   lemmetizing the words: changes words to its basic form accodring to english dictionary\n",
    "\n",
    "*   We used here 2 different techniques to analyse the words present in the corpus. Count vetorizer which represent words by its frequency in each document. and Tf-IDF which represnet each word with its frequency in each document and matched with its presance in the other documents whcih gives each word more or less importance depending on its presance in each document\n",
    "\n",
    "*   Random forest classification was used to identify which document in the test set belongs to which sentimental category\n",
    "\n",
    "*   There was no siginificant difference in accuracy using both vectorization techniques though it was expected that Tf-IDF show more accuracy compared to count vectroizer. These results are influenecd by the skewness in the data and by the model used.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CI80n64BGzEJ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Copy of Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
